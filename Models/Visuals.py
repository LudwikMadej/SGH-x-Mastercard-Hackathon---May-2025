import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.metrics import recall_score,f1_score,precision_score,accuracy_score
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.metrics import classification_report

import matplotlib.pyplot as plt

def aucPlot(fpr, tpr, roc_auc):
    """
    Displays a ROC (Receiver Operating Characteristic) curve with the AUC (Area Under the Curve) value.

    Parameters
    ----------
    fpr : array-like
        False Positive Rates corresponding to different classification thresholds.

    tpr : array-like
        True Positive Rates corresponding to different classification thresholds.

    roc_auc : float
        Area Under the Curve (AUC) value representing the model's performance.

    Description
    -----------
    This function plots the ROC curve for a binary classification model.
    It also includes a reference diagonal line representing random guessing
    and displays the AUC score in the legend.

    The ROC curve helps visualize the trade-off between the true positive rate
    and false positive rate at various thresholds.
    """
    plt.figure()
    plt.plot(fpr, tpr, label=f"ROC curve (AUC = {roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], linestyle="--", color="gray") 
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.grid(True)
    plt.legend(loc="lower right")
    plt.show()


def PredictionQualityInfo(y_pred, y_test):
    """
    Displays a confusion matrix heatmap and prints the classification report
    to evaluate the quality of predictions in a classification task.

    Parameters
    ----------
    y_pred : array-like
        Predicted labels generated by the classifier.

    y_test : array-like
        True labels from the test dataset.

    Description
    -----------
    This function visualizes the confusion matrix using a heatmap and prints the
    detailed classification report including precision, recall, F1-score, and support
    for each class.

    The heatmap includes annotations for easier interpretation and labels for both axes
    to distinguish between predicted and actual classes.

    Intended for use in evaluating binary classification models, such as fraud detection.
    """
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=["Not Fraud", "Fraud"],
                yticklabels=["Not Fraud", "Fraud"])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.tight_layout()
    plt.show()
    print("Classification Report:\n", classification_report(y_test, y_pred, zero_division=0))


def RecallTresholdPlot(y_predict_proba, y_final):
    """
    Plots performance metrics (Recall, F1 Score, Precision, Accuracy) as a function
    of classification threshold values.

    Parameters
    ----------
    y_predict_proba : array-like
        Predicted probabilities for the positive class (e.g., from model.predict_proba).

    y_final : array-like
        True binary class labels.

    Description
    -----------
    This function evaluates how different classification thresholds affect key
    performance metrics: Recall, F1 Score, Precision, and Accuracy.

    For thresholds ranging from 0 to 1 (with a step of 0.01), it binarizes predictions
    and computes the metrics. It then displays four line plots to help visualize
    the trade-offs between these metrics.

    Useful for choosing an optimal threshold, especially in imbalanced classification tasks.
    """
    thresholds = np.arange(0, 1, 0.01)
    RecallVector = np.zeros_like(thresholds)
    F1Vector = np.zeros_like(thresholds)
    PrecisionVector = np.zeros_like(thresholds)
    AccuracyVector = np.zeros_like(thresholds)

    for i, threshold in enumerate(thresholds):
        y_pred = np.array(y_predict_proba > threshold)
        RecallVector[i] = recall_score(y_final, y_pred)
        F1Vector[i] = f1_score(y_final, y_pred,zero_division=0)
        PrecisionVector[i] = precision_score(y_final, y_pred)
        AccuracyVector[i] = accuracy_score(y_final, y_pred)

    fig, axs = plt.subplots(2, 2, figsize=(16, 10))

    axs[0, 0].plot(thresholds, RecallVector)
    axs[0, 0].set_title("RECALL")
    axs[0, 0].set_xlabel("Threshold")
    axs[0, 0].set_ylabel("Score")
    axs[0, 0].grid()

    axs[1, 0].plot(thresholds, F1Vector)
    axs[1, 0].set_title("F1 SCORE")
    axs[1, 0].set_xlabel("Threshold")
    axs[1, 0].set_ylabel("Score")
    axs[1, 0].grid()

    axs[0, 1].plot(thresholds, PrecisionVector)
    axs[0, 1].set_title("PRECISION")
    axs[0, 1].set_xlabel("Threshold")
    axs[0, 1].set_ylabel("Score")
    axs[0, 1].grid()

    axs[1, 1].plot(thresholds, AccuracyVector)
    axs[1, 1].set_title("ACCURACY")
    axs[1, 1].set_xlabel("Threshold")
    axs[1, 1].set_ylabel("Score")
    axs[1, 1].grid()

    plt.tight_layout()
    plt.show()
